{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJIXRteYHZaYYAzwdCdtqG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# [Seq2Seq and Attention](https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html)"],"metadata":{"id":"FD2WNM5gIU6Q"}},{"cell_type":"markdown","source":["- Vanilla RNN\n","> 짧은 시퀀스에 대해서만 효과적임.\n","- LSTM\n","> 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정합니다. 즉, RNN보다 긴 시퀀스의 입력에서도 효과적임. \n","- GRU\n","> LSTM에서 출력,입력,망각(삭제) 게이트라는 3개의 게이트가 존재했다면, GRU는 업데이트 게이트와 리셋 게이트 두 가지 게이트만 존재합니다."],"metadata":{"id":"P8fTR9aXIW0A"}},{"cell_type":"markdown","source":["## [Sequence To Sequence](https://heekangpark.github.io/nlp/attention)"],"metadata":{"id":"HhvTGYLmIaKD"}},{"cell_type":"markdown","source":["### Seq2Seq 동작 방법"],"metadata":{"id":"81gCOeLiIfXZ"}},{"cell_type":"markdown","source":["- Seq2Seq 모델은 '시퀀스를 받아들이는 부분'과 '시퀀스를 출력하는 부분'을 분리한 것이 특징이다.    \n","- 이때 시퀀스를 받아들이는 부분(왼쪽 주황색 RNN모듈)을 인코더(encoder), 시퀀스를 출력하는 부분(오른쪽 녹색 RNN모듈)을 디코더(decoder)라 한다.    \n","- 인코더는 입력 시퀀스(원문)를 받아들여 컨텍스트 벡터(context vector)라 불리는 고정된 크기의 벡터로 변환한다. 디코더는 인코더가 생성한 컨텍스트 벡터를 받아 출력 시퀀스(번역문)를 출력한다.  \n","  \n","아래는 인코더와 디코더의 동작을 좀더 자세하게 설명하였다."],"metadata":{"id":"io5NS2dpIgaW"}},{"cell_type":"markdown","source":["#### Encoder\n","1. 인코더의 은닉 상태를 적절한 값(ex. 영벡터)으로 초기화한다.\n","2. 매 시점(time step)원문의 단어(token)가 입력되면(단어의 임베딩이 입력되면) 인코더는 이를 이용해 은닉 상태를 업데이트를 한다.\n","3. 입력 시퀀스의 끝까지 이 과정을 반복하면 인코더의 최종 은닉 상태는 입력 시퀀스의 정보를 압축 요약한 정보를 담고 있게 된다.\n","4. 이 마지막 시점에서의 인코더 은닉 상태를 컨텍스트 벡터라고 하고, 이 값은 디코더로 넘어간다."],"metadata":{"id":"PlnrgFCgIkRh"}},{"cell_type":"markdown","source":["#### Decoder\n","1. 디코더는 전달받은 컨텍스트 벡터로 자신의 은닉 상태를 초기화한다.\n","2. 그리고 매 시점 자신이 바로 직전 시점에 출력했던 단어를 입력으로 받아, 자신의 은닉 상태를 업데이트하고, 이를 이용해 다음 단어를 예측한다. (최초 시점에서는 시퀀스 시작을 의미하는 `<sos>` 토큰(Start Of Sequence)을 입력으로 받는다.) \n","3. 이 과정을 정해진 반복 횟수 또는 시퀀스 끝을 나타내는 `<eos>` 토큰(End Of Sequence)이 나올 때까지 수행한다."],"metadata":{"id":"KkvGx_gbIm0A"}},{"cell_type":"markdown","source":["### Seq2Seq 한계"],"metadata":{"id":"NN7bxu9sIpyo"}},{"cell_type":"markdown","source":["Seq2Seq 모델은 번역(translation), 챗봇 등의 task에서 높은 성능을 보였다. 하지만 Seq2Seq 모델은 커다란 한계가 있었다.\n","\n","- 입력 시퀸스의 모든 정보를 하나의 고정된 크기의 벡터(컨텍스트 벡터)에 다 압축 요약하려 하다 보니 정보의 손실이 생길 수밖에 없다. 특히 시퀸스의 길이가 길다면 정보의 손실이 더 커진다.\n","- RNN 구조로 만들어진 모델이다 보니, 필연적으로 gradient vaninshing/exploding 현상이 발생한다."],"metadata":{"id":"ApvdPNMOIqpY"}},{"cell_type":"markdown","source":["## [Attention Mechanism](https://blog.floydhub.com/attention-mechanism/)"],"metadata":{"id":"vl-YinpWIu2Q"}},{"cell_type":"markdown","source":["어텐션 메커니즘은 Seq2Seq 모델의 문제점을 개선하기 위해 제안되었다.   \n","\n","구체적으로, 어텐션 메커니즘은 다음을 가정한다.  \n","> 디코더가 단어 X를 출력하기 직전의 디코더 은닉 상태는 인코더가 입력 시퀀스에서 X와 연관이 깊은 단어를 읽은 직후의 인코더 은닉 상태와 유사할 것이다.   \n","\n","예를 들어 영어 문장 \"I am a student.\"을 한국어 문장 \"나는 학생이다.\"로 번역하는 상황을 생각해 보자. 출력 시퀀스의 단어 \"학생\"은 입력 시퀀스의 단어 \"I\", \"am\", \"a\", \"student\", \".\" 중에서 \"student\"와 연관이 깊다.    \n","이때 `어텐션 메커니즘`은 디코더가 \"학생\"을 출력하기 직전의 은닉 상태는 인코더가 \"student\"를 입력받은 직후의 은닉 상태와 유사할 것이라고 가정한다.    \n","따라서 인코더가 \"student\"를 입력받은 직후의 은닉 상태에 조금 더 \"집중\"하면, 훨씬 더 품질 높은 번역을 만들 수 있을 것이다."],"metadata":{"id":"mY4xQm1zIvuY"}},{"cell_type":"markdown","source":["### Attention 동작 방법"],"metadata":{"id":"MgTQuMLsIzxw"}},{"cell_type":"markdown","source":["어텐션 머커니즘의 동작 과정은 query, key, value라는 이름으로 일반화할 수 있다.  \n","- 유사도(similarity): 스코어 함수를 이용하여 query와 각 key들 간의 어켄션 스코어를 계산한다.\n","- 정규화(normalization): softmax 함수를 이용하여 query와 각 key들 간의 어텐션 스코어들을 어텐션 분포로 변환한다.  \n","- 가중합(weighted sum): 어텐션 분포를 이용해 각 value들의 가중합(어텐션 값)을 구한다.   \n","\n","즉 어텐션 연산은 query에 대해 value들을 \"요약\"하는 것이다. 다르게 말하면, 어텐션 연산은 어떤 value에 \"집중\"할지 결정하는 것이다. 어텐션 연산이 수행되면, 중요한(=query와 유사도가 높은 key를 가진) value에 더 \"집중\"하게 된다"],"metadata":{"id":"u007u9GFI1eA"}},{"cell_type":"markdown","source":["### 스코어 함수"],"metadata":{"id":"mpf7_mL4I5Zm"}},{"cell_type":"markdown","source":["query와 key 사이의 유사도를 구하는 함수를 스코어 함수라고 한다.   \n","아래 이미지는 유명한 스코어 함수들을 정리한 표이다."],"metadata":{"id":"8Al_SodQI6Lh"}},{"cell_type":"markdown","source":["## Seq2Seq with Attention 동작 방법"],"metadata":{"id":"2O4d9uWLI9Sn"}},{"cell_type":"markdown","source":["1. 어느 시점의 인코더 은닉 상태에 조금 더 \"집중\"해야 하는지 찾기 위해, 현재 디코더의 은닉 상태와 매 시점 인코더의 은닉 상태들 간 \"유사도\"를 계산한다.  \n","2. 이 유사도를 확률의 형태로 바꾸고, 그 값에 따라 인코더 은닉 상태들의 가중합(weighted sum)을 구해 \"보정된 컨텍스트 벡터\"를 구한다.  \n","3. \"보정된 컨텍스트 벡터\"을 이용해 다음 단어를 예측한다.  "],"metadata":{"id":"t0_qNSqgI-MX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQXsm2PaINy6","executionInfo":{"status":"ok","timestamp":1678848180099,"user_tz":-540,"elapsed":3849,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"40a267d9-d99e-4feb-f755-869d7db7bbb5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":1}],"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","source":["문자 단위 RNN 튜토리얼에서 사용된 문자 인코딩과 유사하게, 언어의 각 단어들을 One-Hot Vector로 표현합니다. 언어마다 수십 개의 문자가 있어서 인코딩 벡터는 매우 큽니다.   \n","  \n","그러나 우리는 약간의 트릭을 써서 언어 당 수천 단어만 사용하도록 데이터를 다듬을 것입니다.  "],"metadata":{"id":"Exsi7vxvJiDu"}},{"cell_type":"markdown","source":["나중에 네트워크의 입력 및 목표로 사용하려면 단어 당 고유 번호가 필요합니다. 이 모든 것을 추적하기 위해 우리는 `단어-색인(word2index)`과 `색인-단어(index2word)` 사전, 그리고 나중에 희구 단어를 대체하는데 사용할 각 `단어의 빈도(word2count)`를 가진 Lang 클라스를 만듭니다.  \n","  \n","- SOS token: Start Of String Token\n","- EOS token: End Of String Token"],"metadata":{"id":"XQ-ViwYKJm7Y"}},{"cell_type":"code","source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","  def __init__(self, name):\n","    self.name = name\n","    self.word2index = {}\n","    self.word2count = {}\n","    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","    self.n_words = 2\n","\n","  def addSentence(self, sentence):\n","    for word in sentence.split(' '):\n","      self.addWord(word)\n","\n","  def addWord(self, word):\n","    if word not in self.word2index:\n","      self.word2index[word] = self.n_words \n","      self.word2count[word] = 1\n","      self.index2word[self.n_words] = word\n","      self.n_words += 1\n","    else:\n","      self.word2count[word] += 1  "],"metadata":{"id":"Pn2FnS54IVuM","executionInfo":{"status":"ok","timestamp":1678848361109,"user_tz":-540,"elapsed":6,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["파일은 모두 유니코드로 되어있어 간단하게하기 위해 유니코드 문자를 ASCII로 변환하고, 모든 문자를 소문자로 만들고, 대부분의 구두점을 지워줍니다."],"metadata":{"id":"fzRVBzCUKRH4"}},{"cell_type":"code","source":["# 유니 코드 문자열을 일반 ASCII로 변환하십시오.\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# 소문자, 다듬기, 그리고 문자가 아닌 문자 제거\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s"],"metadata":{"id":"4Z1n9vgtKP-w","executionInfo":{"status":"ok","timestamp":1678848424041,"user_tz":-540,"elapsed":287,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/data')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yf7lzYG6KfJN","executionInfo":{"status":"ok","timestamp":1678848464347,"user_tz":-540,"elapsed":18068,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"b7340b2e-cca6-4790-d697-c4e28b07c53f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/data\n"]}]},{"cell_type":"code","source":["def readLangs(lang1, lang2, reverse=False):\n","  print(\"Reading lines...\")\n","\n","  DATA_PATH = \"/content/data/MyDrive/dev/2.deep learning/4. NLP/data/\"\n","  lines = open(DATA_PATH+'%s-%s.txt' % ('eng', 'fra'), encoding='uft-8').\\\n","      read().strip.split('\\n')\n","\n","  # 모든 줄을 쌍으로 분리하고 정규화\n","  pairs = [[normalizeString(s) for s in l.split('\\t')] for i in lines]\n","\n","  # 쌍을 뒤집고 Lang 인스턴스 생성\n","  if reverse:\n","    pairs = [list(reversed(p)) for p in pairs]\n","    input_lang = Lang(lang2)\n","    output_lang = Lang(lang1)    \n","  else:\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)  \n","  \n","  return input_lang, output_lang, pairs"],"metadata":{"id":"gdf4NhWZKkpv","executionInfo":{"status":"ok","timestamp":1678848712496,"user_tz":-540,"elapsed":528,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["MAX_LENGTH = 10 #최대 길이\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"metadata":{"id":"5d__MqXWLllv","executionInfo":{"status":"ok","timestamp":1678848726593,"user_tz":-540,"elapsed":4,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def prepareData(lang1, lang2, reverse=False):\n","  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","  print(\"Read %s sentence pairs\" % len(pairs))\n","  pairs = filterPairs(pairs)\n","  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","  print(\"Counting words....\")\n","  for pair in pairs:\n","      input_lang.addSentence(pair[0])\n","      output_lang.addSentence(pair[1])\n","  print(\"Counted words:\")\n","  print(input_lang.name, input_lang.n_words)\n","  print(output_lang.name, output_lang.n_words)\n","  return input_lang, output_lang, pairs"],"metadata":{"id":"uebvv4viLoql","executionInfo":{"status":"ok","timestamp":1678849816090,"user_tz":-540,"elapsed":742,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n","print(random.choice(pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"id":"SUtLp4elMfkS","executionInfo":{"status":"error","timestamp":1678849859509,"user_tz":-540,"elapsed":5,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"fbab1be0-78a8-483c-86f5-408ada17090b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n"]},{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-08bcc5398987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fra'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-f5cb61801d46>\u001b[0m in \u001b[0;36mprepareData\u001b[0;34m(lang1, lang2, reverse)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadLangs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read %s sentence pairs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilterPairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trimmed to %s sentence pairs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-8f3680e559db>\u001b[0m in \u001b[0;36mreadLangs\u001b[0;34m(lang1, lang2, reverse)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/data/MyDrive/dev/2.deep learning/4. NLP/data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'%s-%s.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fra'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uft-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: unknown encoding: uft-8"]}]},{"cell_type":"markdown","source":["# 2.Seq2Seq Model"],"metadata":{"id":"gSFt2npJMryE"}},{"cell_type":"markdown","source":["Sequence to Sequence network, 또는 Seq2Seq 네트워크, 또는 encoder Decoder network는 인코더 및 디코더라고 하는 두 개의 RNN으로 구성된 모델입니다.   \n","  \n","인코더는 입력 시퀀스를 읽고 단일 벡터를 출력하고, 디코더는 해당 벡터를 읽어 출력 시퀀스를 생성합니다."],"metadata":{"id":"6TJ0jqSjM376"}},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"JXXHTuF7M6LU"}},{"cell_type":"markdown","source":["Seq2Seq 네트워크의 인코더는 입력 문장의 모든 단어에 대해 어떤 값을 출력하는 RNN입니다. 모든 입력 단어에 대해 인코더는 벡터와 은닉 상태를 출력하고 다음 입력 단어를 위해 그 은닉 상태를 사용합니다."],"metadata":{"id":"Pzlv14SmM-P5"}},{"cell_type":"code","source":["class EncoderRNN(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(EncoderRNN, self).__init__()\n","    self.hidden_size = hidden_size\n","\n","    self.embedding = nn.Embedding(input_size, hidden_size)\n","    self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","  def forward(self, input, hidden):\n","    embedded = self.embedding(input).view(1, 1, -1)\n","    output = embedded \n","    output, hidden = self.gru(output, hidden)\n","    return output, hidden\n","\n","  def initHidden(self):\n","    return torch.zeros(1, 1, self.hidden_size, device=device)   "],"metadata":{"id":"xN-Yq_liMpH0","executionInfo":{"status":"ok","timestamp":1678849242972,"user_tz":-540,"elapsed":304,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Decoder\n","> 디코더는 인코더 출력 벡터를 받아서 번역을 생성하기 위한 단어 시퀀스를 출력합니다."],"metadata":{"id":"b2U21NgoNoRr"}},{"cell_type":"markdown","source":["### 간단한 디코더\n","가장 간단한 Seq2Seq 디코더는 인코더의 마지막 출력만을 이용합니다. 이 마지막 출력은 전체 시퀀스에서 문맥을 인코드하기 때문에 문맥 벡터(context vector)로 불립니다. 이 문맥 벡터는 디코더의 초기 인닉 상태로 사용됩니다."],"metadata":{"id":"eiuTV0R8NtTJ"}},{"cell_type":"code","source":["class DecoderRNN(nn.Module):\n","  def __init__(self, hidden_size, output_size):\n","    super(DecoderRNN, self).__init__()\n","    self.hidden_size = hidden_size\n","\n","    self.embedding = nn.Embedding(output_size, hidden_size)\n","    self.gru = nn.GRU(hidden_size, hidden_size)\n","    self.out = nn.Linear(hidden_size, output_size)\n","    self.softmax = nn.LogSoftmax(dim=1)\n","\n","  def forward(self, input, hidden):\n","    output = self.embedding(input).view(1, 1, -1)\n","    output = F.relu(output)\n","    output, hidden = self,gru(output, hidden)\n","    output = self.softmax(self.out(output[0]))\n","    return output, hidden\n","\n","  def initHidden(self):\n","    return torch.zeros(1, 1, self.hidden_size, device=device)    "],"metadata":{"id":"tju6aIe4NnMu","executionInfo":{"status":"ok","timestamp":1678849458551,"user_tz":-540,"elapsed":8,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Attention 디코더"],"metadata":{"id":"fAzrdAbbOd1x"}},{"cell_type":"markdown","source":["문맥 벡터(context vector)만 인코더와 디코더 사이로 전달 된다면, 단일 벡터가 전체 문장을 인코딩 해야하는 부담을 가지게 됩니다.   \n","  \n","Attention은 디코더 네트워크가 자기 출력의 모든 단계에서 인코더 출력의 다른 부분에 집중 할 수 있게 합니다."],"metadata":{"id":"Xc__xySjOgrH"}},{"cell_type":"code","source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"metadata":{"id":"bsKwBcx7Ob6E","executionInfo":{"status":"ok","timestamp":1678849493622,"user_tz":-540,"elapsed":5,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# 3.학습"],"metadata":{"id":"_dAvn_VZOntQ"}},{"cell_type":"markdown","source":["## 학습 데이터 준비\n","> 학습을 위해서, 각 쌍마다 입력 Tensor(입력 문장의 단어 주소)와 목표 Tensor(목표 문장의 단어 주소)가 필요합니다. 이 벡터들을 생성하는 동안 두 시퀀스에 EOS 토큰을 추가 합니다."],"metadata":{"id":"NkJN7ERkOpgB"}},{"cell_type":"code","source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)"],"metadata":{"id":"poSBQcViOkas","executionInfo":{"status":"ok","timestamp":1678849582428,"user_tz":-540,"elapsed":3,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## 모델 학습\n","학습을 위해서 인코더에 입력 문장을 넣고 모든 출력과 최신 은닉 상태를 추적합니다. 그런 다음 디코더에 첫 번재 입력으로 `<SOS>` 토큰과 인코더의 마지막 은닉 상태가 첫 번째 은닉 상태로 제공됩니다.   "],"metadata":{"id":"pjGrGa4RO7F4"}},{"cell_type":"code","source":["teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing 포함: 목표를 다음 입력으로 전달\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Teacher forcing 미포함: 자신의 예측을 다음 입력으로 사용\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # 입력으로 사용할 부분을 히스토리에서 분리\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"],"metadata":{"id":"XGXHqg5SO560","executionInfo":{"status":"ok","timestamp":1678849691603,"user_tz":-540,"elapsed":442,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"],"metadata":{"id":"grKhrlKnPUrf","executionInfo":{"status":"ok","timestamp":1678849704038,"user_tz":-540,"elapsed":2,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # 주기적인 간격에 이 locator가 tick을 설정\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"],"metadata":{"id":"h36b6vZhPX4q","executionInfo":{"status":"ok","timestamp":1678849711236,"user_tz":-540,"elapsed":292,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["> 전체 학습 과정\n","- 타이머 시작\n","- optimizers와 criterion 초기화\n","- 학습 쌍의 세트 생성\n","- 도식화를 위한 빈 손실 배열 시작"],"metadata":{"id":"xf-xKcX3Pbb2"}},{"cell_type":"code","source":["def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # print_every 마다 초기화\n","    plot_loss_total = 0  # plot_every 마다 초기화\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    training_pairs = [\n","        tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","\n","        loss = train(input_tensor, target_tensor, encoder,\n","                    decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"],"metadata":{"id":"Mb38ovsFPZhP","executionInfo":{"status":"ok","timestamp":1678849731212,"user_tz":-540,"elapsed":300,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## 4.평가"],"metadata":{"id":"a1TReUjOPfhO"}},{"cell_type":"markdown","source":["평가는 대부분 학습과 동일하지만 목표가 없으므로 각 단계마다 디코더의 예측을 되돌려 전달합니다.     \n","단어를 예측할 때마다 그 단어를 출력 문자열에 추가합니다. 만약 `EOS` 토큰을 예측하면 거기에서 멈춥니다."],"metadata":{"id":"aafKoLYFPii2"}},{"cell_type":"code","source":["def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n","                                                    encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","        decoder_attentions = torch.zeros(max_length, max_length)\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            decoder_attentions[di] = decoder_attention.data\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words, decoder_attentions[:di + 1]"],"metadata":{"id":"Cv8By_kFPeVN","executionInfo":{"status":"ok","timestamp":1678849762344,"user_tz":-540,"elapsed":2,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["학습 세트에 있는 임의의 문장을 평가하고 입력, 목표 및 출력을 하여 주관적인 품질 판단을 내릴 수 있습니다."],"metadata":{"id":"beqN1hrkPoZV"}},{"cell_type":"code","source":["def evaluateRandomly(encoder, decoder, n=10):\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words, attentions = evaluate(encoder, decoder, pair[0])\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')"],"metadata":{"id":"XN18FaNXPmFK","executionInfo":{"status":"ok","timestamp":1678849779223,"user_tz":-540,"elapsed":298,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## 5.학습과 평가"],"metadata":{"id":"ieWbWD5-Pqu2"}},{"cell_type":"code","source":["import time\n","\n","hidden_size = 256\n","encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n","\n","trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"L19ChHyPPqE2","executionInfo":{"status":"error","timestamp":1678849793315,"user_tz":-540,"elapsed":6,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"240e24f1-e2b4-4208-d068-31c076c23d81"},"execution_count":18,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-d51d359c04cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_lang' is not defined"]}]},{"cell_type":"code","source":["evaluateRandomly(encoder1, attn_decoder1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172},"id":"RA2idRapPtWG","executionInfo":{"status":"error","timestamp":1678849903540,"user_tz":-540,"elapsed":409,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"06256027-92f0-439e-816d-b04d5f043f12"},"execution_count":22,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-752866089bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'encoder1' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lI7KYMiUQIWa"},"execution_count":null,"outputs":[]}]}